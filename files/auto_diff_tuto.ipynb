{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "!conda install --yes --prefix {sys.prefix} pytorch torchvision cpuonly -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch Tutorial\n",
    "=============\n",
    "\n",
    "This tutorial presents Pytorch, a Python-based scientific computing package developed for deep-learning that implements automatic-differentiation in a user-friendly framework. It has also the advantage to enable computations and derivations on Graphics Processing Units (GPUs) which allows fast training of convolutional deep networks\n",
    "\n",
    "This tutorial  is partly based on the offical tutorial that can be found [here](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors\n",
    "------\n",
    "\n",
    "The central objects in Pytorch are tensors. Tensors are similar to NumPyâ€™s ndarrays, with the addition being that\n",
    "Tensors can also be used on a GPU to accelerate computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a 5x3 matrix, uninitialized:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a randomly initialized matrix:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a matrix filled with zeros or ones\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(5, 3)\n",
    "y = torch.ones(5, 3)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a tensor directly from a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([5.5, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or create a tensor based on an existing tensor, here we use randn_like but zeros_like or ones_like work too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(5, 3)\n",
    "x = torch.randn_like(x, dtype=torch.float)\n",
    "print(x)                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get its size:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operations on tensors\n",
    "------\n",
    "\n",
    "There are multiple syntaxes for operations. In the following\n",
    "example, we will take a look at the addition operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Element-wise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x*y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot-products (dot) Matrix-vector products (mv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(3)\n",
    "b = torch.rand(3)\n",
    "print(a.dot(b))\n",
    "print(x.mv(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sums of elements, (Euclidean) norm (square root of the sum of the squred elements of the tensor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.sum(x))\n",
    "print(torch.norm(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read later:**\n",
    "\n",
    "\n",
    "  100+ Tensor operations, including transposing, indexing, slicing,\n",
    "  mathematical operations, linear algebra, random numbers, etc.,\n",
    "  are described\n",
    "  here <https://pytorch.org/docs/torch>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use standard NumPy-like indexing with all bells and whistles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resizing: If you want to resize/reshape tensor, you can use ``torch.view``:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a one element tensor, use ``.item()`` to get the value as a\n",
    "Python number\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-place operations: Any operation that mutates a tensor in-place is post-fixed with an ``_``.\n",
    "    For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adds x to y\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "CUDA Tensors\n",
    "------------\n",
    "\n",
    "Pytorch can handle gpu computations by using cuda if you have access to gpus (which is not the case in this tutorial). To handle gpu computations you need to transfer the data to the gpu devide as done below. Gpus are useful to get faster implementations of some operations such as convolutions of images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automatic differentiation\n",
    "=========\n",
    "\n",
    "Follow the slides until slide 11. Below is a first scalar example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate variable with flag to record any computation involving w0\n",
    "w0 = torch.rand(1, requires_grad = True)\n",
    "\n",
    "# Compute logistic loss on (x,y) = (3.5, 1)\n",
    "out = torch.log(1+torch.exp(-3.5*w0))\n",
    "\n",
    "# Backpropagate gradients of any input involved in out\n",
    "out.backward()\n",
    "\n",
    "# Derivative is recorded in the variable\n",
    "print(w0.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with multivariate functions, slides 11-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data n =100, d = 20\n",
    "X = torch.rand(100, 20)\n",
    "\n",
    "w0 = torch.rand(20, requires_grad=True)\n",
    "\n",
    "# Compute logistic loss (mv stands for matrix vector product)\n",
    "out = sum(torch.log(1+ torch.exp(-X.mv(w0))))\n",
    "\n",
    "# Backpropagate gradients\n",
    "out.backward()\n",
    "\n",
    "print(w0.grad)\n",
    "print(w0.grad.shape) # get 20 dimensional vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally with two parameters introduced at different steps of the computations, slides 13-14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate weights\n",
    "w1 = torch.rand(1, requires_grad = True); \n",
    "w2 = torch.rand(1, requires_grad = True)\n",
    "\n",
    "# Compute square loss of 1-layer DNN with tanh activation on (x,y)=(3.5,1)\n",
    "out = torch.tanh(3.5*w1)\n",
    "out = w2*out\n",
    "out = (1 - out)**2\n",
    "\n",
    "# Backpropagate gradients of any input involved in out\n",
    "out.backward()\n",
    "\n",
    "# Gradients are recorded in the variables\n",
    "print(w1.grad)\n",
    "print(w2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than using backward and having gradients stored implicitly in the variables, you can use torch.autograd.grad. This function directly returns the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(100, 20)\n",
    "\n",
    "w0 = torch.rand(20, requires_grad=True)\n",
    "\n",
    "out = sum(torch.log(1+ torch.exp(-X.mv(w0))))\n",
    "\n",
    "# First argument is the result of the operations, \n",
    "# the second argument is the input from which the gradient is taken\n",
    "# The second argument can take a list of inputs, the output of the function is then a tuple\n",
    "# So in our case the output is a single tuple that we extract\n",
    "grad = torch.autograd.grad(out, w0)[0]\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Pytorch only computes gradients of scalar functions. The reason is that one often does not need to compute the Jacobian matrix itself but the product of the Jacobian with any vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep networks\n",
    "=======\n",
    "Formally, neural networks are composed of $L$ layers. \n",
    "The affine mapping at the $l^{\\mathrm{th}}$ layer\n",
    "is parameterized by the  weights-bias pair $u_l = (W_l, b_l)$. \n",
    "The complete mapping is then a function of $u = (u_1,\\ldots,u_L)$  given by\n",
    "$$\n",
    "\\Phi(x;u) = \\phi_{u_{L}}\\circ \\phi_{u_{L-1}} \\circ \\ldots \\circ \\phi_{u_{1}}(x),\n",
    "$$\n",
    "where, for any $l = 1,\\ldots, L$,\n",
    "$$\n",
    "\\phi_{u_l}(x) =  \\sigma(W_l x +b_l),\n",
    "$$\n",
    "and $\\sigma$ is a non-linear operation such as the point-wise application of the \n",
    "soft-plus function $\\log(1+\\exp(\\beta x))/\\beta$.\n",
    "\n",
    "On an input-output pair $(x,y)$, the error of prediction of $y$ by $\\Phi(x;u)$ is measured by a loss $\\ell(y, \\Phi(x;u))$ such as the squared loss, $\\ell_2(y,\\Phi(x;u))=\\frac{1}{2} \\|y-\\Phi(x;u)\\|_2^2$ or logistic loss and the learning procedure consists of minimizing the empirical loss on the set of input-output training examples $(x_1,y_1),\\ldots,(x_n,y_n)$ leading to the optimization problem\n",
    "$$\n",
    "   \\min_{\\substack{u = (u_1,\\ldots,u_L)}} \\quad \\frac{1}{n} \\sum_{i=1}^n \\ell(y_i,\\Phi(x_i;u)).\n",
    "$$\n",
    "\n",
    "Instantiate a simple network\n",
    "----------------------------------\n",
    "A neural network can easily be defined using the framework provided by [PyTorch](https://pytorch.org/). There, a network is an object defined by\n",
    "* the affine transformations parameterized by $u_1,\\ldots,u_L$ above, that are the attributes of the network , through the constructor `__init__`,\n",
    "* the whole operations made on an input by the network, i.e. the function $\\Phi$ defined above, (so composing the linear transformations with non-linear ones), through the function <code>forward</code>.\n",
    "\n",
    "We will see different linear and non-linear operations in this lab. For the moment we will just use the ones presented before, i.e. affine transformation and the softplus function.\n",
    "\n",
    "To begin with, we will define a simple Multi-Layer Perceptron (MLP) for images with two layers, i.e. parameterized by two linear transformations ($u_1, u_2$). The Mapping $\\Phi$ will then reduce the input (in this case, an image) to a vector, apply the first affine map (such a layer is called fully connected (fc below)), then the soft-plus function as the non-linearity $\\sigma$ and finally apply the second affine map.\n",
    "\n",
    "Below is the code in PyTorch to define this MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn.functional import softplus\n",
    "class MLPNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple multi-layer perceptron (MLP) with one hidden layer\n",
    "    \"\"\"\n",
    "    def __init__(self, size_img, num_hidden_nodes, num_classes):\n",
    "        super(MLPNet, self).__init__()\n",
    "        # First linear map form the space of images represented by \n",
    "        # vectors of size size_img to the dimension of the first layer\n",
    "        self.size_img = size_img\n",
    "        self.fc1 = nn.Linear(size_img, num_hidden_nodes)\n",
    "        # Second linear map from output of layer 1 to the output space (here composed of num_classes)\n",
    "        self.fc2 = nn.Linear(num_hidden_nodes, num_classes)\n",
    "        # Define the non-linearity\n",
    "        # threshold parameter is properly defined in pytorch manual\n",
    "        self.nonlin = lambda x: softplus(x, beta=25, threshold=1)\n",
    "        # self.nonlin = relu # uncomment to use ReLu instead\n",
    "    def forward(self, x):\n",
    "        # Reduce image to a vector:\n",
    "        x = x.view(-1, self.size_img)\n",
    "        # Apply first linear map followed by the non-linearity\n",
    "        x = self.nonlin(self.fc1(x))\n",
    "        # Apply the second linear map\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generation\n",
    "---------------\n",
    "We will use this network to classify handwritten digits from the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) (see examples of data [here](https://oriamathematics.wordpress.com/2016/08/12/binary-classification-with-logistic-regression/)). These are 28x28 images  of digits 0,...,9 (10 classes) represented by a one-hot encoding. That is, image $x_i \\in \\mathbb{R}^{28 \\times 28}$ and label $y_i \\in \\{0, 1\\}^{10}$. Note that $y_i$ is represented by a binary vector with the $k$-th entry equal to $1$ if the image $x_i$ belongs to class $k$ and $0$ otherwise. We define a network with 500 hidden nodes in the first layer and use the square loss. \n",
    "Formally, we require our neural network $\\Phi : \\mathbb{R}^{28 \\times 28} \\to \\mathbb{R}^{10}$ to have 10 nodes in the last (output) layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate neural network and loss\n",
    "net = MLPNet(28*28, 500, 10)\n",
    "loss_fn = nn.MSELoss() # MSE = mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loading and the preprocessing of the data is handled by the library torchvisions that is provided in Pytorch. It gives access to a large library of datasets and preprocessing tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up dataset and dataloader see Pytorch documentation \n",
    "# <https://pytorch.org/tutorials/beginner/data_loading_tutorial.html> for more details\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "# Pre-computed mean and standard deviation.\n",
    "mean_MNIST = 0.1307\n",
    "std_MNIST = 0.3081\n",
    "# Preprocess the data \n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((mean_MNIST,), (std_MNIST,))])\n",
    "# Load MNIST training set with one hot encoding of labels \n",
    "# (i.e. y is normally given as a digit between 0 and 9 such as y=6 \n",
    "# and we transform it into a vector of 10 coordinates with a one on the coordinate of the digit\n",
    "# here a one on the 6th coordinate if y=6)\n",
    "class MakeOneHot(object):\n",
    "    \"\"\"\n",
    "    Transform ordinary output into corresponding binary vector\n",
    "    \"\"\"\n",
    "    def __init__(self, nb_classes):\n",
    "        self.nb_classes = nb_classes\n",
    "\n",
    "    def __call__(self, target):\n",
    "        out = torch.eye(self.nb_classes)\n",
    "        return out[target].view(-1)\n",
    "\n",
    "target_transform = MakeOneHot(10)\n",
    "trainset = torchvision.datasets.MNIST(root='./data/MNIST', train=True, target_transform=target_transform,\n",
    "                                      transform=transform, download=True)\n",
    "# Dataloader will output batch of samples form the data set, randomly chosen by suffling the dataset \n",
    "# To load a batch of batch_size sample at each time we will use \n",
    "# dataloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "We will now compare different optimization methods to solve the training problem that can be simply written as\n",
    "$$\n",
    "\\min_u \\, f(u) = \\frac{1}{n}\\sum_{i=1}^n f_i(u)\n",
    "$$\n",
    "where $f_i(u) = \\ell(y_i,\\Phi(x_i;u))$ is the loss encountered by the set of parameters on the ith training sample. \n",
    "\n",
    "We focus on first order algorithms that use gradients on the problem. To compute them, notice that each $f_i$ is a composition of functions, such that its gradient is given by the chain rule. Precisely, the gradient of $f_i$ with respect to the $l$th variables of the network $u_l$ will depend on the gradient of the next layers for $l' = l+1,\\ldots,L$. \n",
    "\n",
    "Practically in the PyTorch framework, given a sample, the neural net records all the operations it computes from the input $x_i$ until the value of the loss with the parameters of the network. From these operations that it recorded (from the definition of the loss and the <code>forward</code> function above), it derives the gradient of the corresponding function $f_i$.\n",
    "\n",
    "The following code computes the gradient of the network with respect to as many samples as it gets (in x, y below).\n",
    "\n",
    "__Explanation__: \n",
    "- The various parameters of a neural network (net), represented by an object of type nn.Module may be accessed individually via their names, e.g. `net.fc1.weight` or `net.fc2.bias` in `MLPNet` above, or via the iterator `net.parameters()`. \n",
    "- The function below returns the loss, which can then be used for aggregation and book-keeping, and a reference to the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stoc_grad_(net, loss_fn, x, y):\n",
    "    \"\"\"\n",
    "    The function computes gradient of `loss_fn(net(x), y)`\n",
    "    w.r.t. parameters of `net`, where `x` is the input `y` is the label.\n",
    "    Returns the function value `loss_fn(net(x), y)` and the gradient.\n",
    "    \"\"\"\n",
    "    loss = loss_fn(net(x), y) # compute loss\n",
    "    # Return reference to gradient with respect to all the parameters of the network\n",
    "    # The parameters of the network correspond here to all affine maps defiend above.\n",
    "    # The W and b of each affine layer is recorded when the network is initialized \n",
    "    # and added to the list of parameters that can be called like here \n",
    "    grad = torch.autograd.grad(loss, [param for param in net.parameters()])\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)\n",
    "For large data sets, computing the whole gradient may be costly. Therefore one can rather use approximations of it: compute the gradient of a random subset $I$ of size $m$ of the samples, \n",
    "$$\n",
    "\\tilde \\nabla f(u) = \\frac{1}{m}\\sum_{i \\in I} \\nabla f_i(u)\n",
    "$$\n",
    "By drawing the subsets at random, we ensure that this approximate gradient is a unbiased estimate of the true gradient, i.e. $E(\\tilde \\nabla f(u)) = \\nabla f(u)$. Random subsets of more than one sample are called *mini-batches*. \n",
    "\n",
    "From this estimate of the gradient we can apply the stochastic gradient descent that starts from a given $u_0$ and follows the rule\n",
    "$$\n",
    "u_{k+1} = u_k - \\gamma_k \\tilde \\nabla f(u_k) \n",
    "$$\n",
    "where $\\gamma_k$ is the step-size (learning rate).\n",
    "\n",
    "The following function implements SGD with a constant learning rate `lr`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "def sgd(net, loss_fn, dataloader, lr, num_epochs):\n",
    "    \"\"\"\n",
    "        run SGD with constant learning rate\n",
    "        - For inputs x,y, the function is defined as `loss_fn(net(x), y)`.\n",
    "        - `dataloader` is an iterator over the examples $\\{x_i, y_i\\}_{i=1}^n$.\n",
    "        - `lr` is the (constant) learning rate $gamma$ used by SGD.\n",
    "        - SGD is run for `num_epochs` number of passes through the dataset.\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    print('Epoch\\tAvg loss\\tTime')\n",
    "    for epoch in range(num_epochs):\n",
    "        t1 = time.time()\n",
    "        for i, (images, labels) in enumerate(dataloader):\n",
    "            # dataloader here gives randomly sampled images and labels\n",
    "            loss, grad = stoc_grad_(net, loss_fn, images, labels)\n",
    "            losses += [loss.item()]\n",
    "            with torch.no_grad():\n",
    "                for var, grad_var in zip(net.parameters(), grad):\n",
    "                    # grad_var is a pointer to gradient of loss w.r.t. var\n",
    "                    updated_var = var - lr*grad_var\n",
    "                    var.copy_(updated_var) # copy updated_var into var\n",
    "            if i % 100 == 99: # Logging\n",
    "                print('{:.2f}\\t{:.4f}\\t\\t{:.2f}'.format(\n",
    "                    epoch + i / len(dataloader), # number of passes\n",
    "                    np.asarray(losses).mean(),  # mean loss seen this epoch\n",
    "                    time.time() - t1))\n",
    "        losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology to find hyper-parameters of stochastic gradient descent\n",
    "\n",
    "This algorithm can be proven to converge to a stationary point under mild assumptions on the smoothness of the function. This depends obviously on the choice of the step-sizes that shall tend to zero to ensure the reduction of the variance of the estimates. Theory tells that a step-size of $\\gamma_k \\propto 1/\\sqrt{k+1}$ is optimal, practitioners often use heuristics that are found to work better in practice.\n",
    "\n",
    "Precisely, practitioners run stochastic gradient descent with a constant step-size $\\gamma$ until a given stopping criterion, then they restart the algorithm from the final point using a new constant step-size $\\delta \\gamma$, reduced by a fixed factor $\\delta < 1$, and repeat the process. This strategy requires three hyper-parameters: \n",
    "- initial learning rate $\\gamma_0$, \n",
    "- stopping criterion,\n",
    "- learning rate decay factor $\\delta$.\n",
    "\n",
    "Let us look at each in turn:\n",
    "- __Initial learning rate $\\gamma_0$__: The initial learning rate $\\gamma_0$ must be large enough so that the algorithm makes good progress initially, but not so large that the optimization diverges. Too small an initial learning rate leads to very slow optimization. \n",
    "- __Stopping Criterion__: Two stopping criteria are commonly used: \n",
    "    - *fixed iteration budget*: Stop after a fixed number $T_{\\mathrm{budget}}$ number of epochs. Here, $T_{\\mathrm{budget}}$  is a hyper-parameter.\n",
    "    - *plateau on a validation set*: Use a validation set to stop when the performance on it (either in loss value or classification accuracy)  has not improved in a fixed number $T_{\\mathrm{patience}}$ number of epochs where $T_{\\mathrm{patience}}$ is a hyper-parameter.\n",
    "- __Decay factor $\\delta$__: As for the learning rate, the decay factor $\\delta$, shouldn't be too small in order to avoid to be quickly stuck.\n",
    "\n",
    "Practitioners use default values for $T_{\\mathrm{budget}}$ or $T_{\\mathrm{patience}}$ and search only for the initial learning rate $\\gamma_0$ and the decay factor $\\delta$. We don't detail here the grid search for the decay factor, simply know that it is commonly searched on a grid $\\delta \\in\\{1/2, 1/4, 1/8\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test SGD\n",
    "Let us now try to experiment some constant learning rates for SGD. \n",
    "Intuitively very large learning rates cause the optimization to diverge\n",
    "i.e. the function value and parameter norms quickly blow up to $+\\infty$ (or you might notice NaNs that arise from arithmetic operation involving $\\infty$).\n",
    "Here we will find a learning rate $\\gamma$ so that SGD with learning rate $c\\gamma$, $c > 1$ (e.g., $10$), diverges \n",
    "immediately (i.e., within a few tens of updates)\n",
    "but doesn't with a learning rate $\\gamma$.\n",
    "\n",
    "For the case of [cross entropy loss (log loss)](https://en.wikipedia.org/wiki/Loss_functions_for_classification#Cross_entropy_loss_(Log_Loss)),\n",
    "which is popular for classification problems, \n",
    "too large a learning rate causes the function value to become large (although not $+\\infty$)\n",
    "and never drop below $\\ln 10 \\approx 2.3$. Note that the dataset MNIST\n",
    "that we consider has 10 classes, and a function value of $\\ln 10$ corresponds \n",
    "to random guessing where each class is assigned a probability of $1/10$.\n",
    "\n",
    "**Exercise 1:**\n",
    "Try to find a good initial learning rate up as follows: find a number $\\gamma_0$ so that \n",
    "SGD works with an initial learning rate of $\\gamma_0$ \n",
    "but diverges with $10\\gamma_0$. \n",
    "Once we have this estimate, $\\gamma_0$ or $\\gamma_0 / 10$ are usually good candidates for initial learning rates.\n",
    "\n",
    "**Exercise 2:**\n",
    "Try different batch-sizes 50, 100, 200. Find the best learning rate for each batch-size. Compare the difference in the speed of the decreasing of the loss for the best learning rate you can find.\n",
    "\n",
    "**Solution 1:**\n",
    "You might have found above that $\\gamma=0.1$ or $\\gamma=1.0$ \n",
    "results in a rapid decrease of the loss while\n",
    "$\\gamma = 10.0$ \n",
    "causes NaNs, a sign of divergence.\n",
    "A smaller step-size of $\\gamma = 0.01$ also works but is much more conservative\n",
    "than the choice $\\gamma = 0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=200\n",
    "dataloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "net = MLPNet(28*28, 500, 10) # Reinitialize the net when using a different learning rate\n",
    "lr = 1e-1 # learning rate\n",
    "sgd(net, loss_fn, dataloader, lr, num_epochs=2) # call SGD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
