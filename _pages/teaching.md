---
layout: archive
title: "Teaching"
author_profile: true
redirect_from:
  - /teaching.html
---

{% include base_path %}

## Lecturer  
**Stochastic Modeling of Scientific Data II**, *STAT517, Winter 2021.*  
*University of Washington, Seattle.*   
Axiomatic definitions of probability; random variables; conditional probability and Bayes' theorem; expectations and variance; classical distributions. Transformations of a single random variable. Markov and Chebyshev's inequality. Weak law of large numbers for finite variance.

**Stochastic Modeling of Scientific Data**, *STAT516, Fall 2020.*  
*University of Washington, Seattle.*  
Covers discrete-time Markov chain theory; inference for discrete-time Markov chains; Monte Carlo methods; missing data; hidden Markov models; and Gaussian Markov random fields.

**Probability II**,  *STAT395 Spring 2020, 2021.*  
*University of Washington, Seattle.*  
Random variables; expectation and variance; laws of large numbers; normal approximation and other limit theorems; multidimensional distributions and transformations.

**Probability I**, *STAT394, Winter 2021.*  
*University of Washington, Seattle.*  
Random variables; expectation and variance; laws of large numbers; normal approximation and other limit theorems; multidimensional distributions and transformations.

**Statistical Learning: Modeling, Prediction, And Computing**,  *STAT538 Winter 2020.*  
*University of Washington, Seattle. Co-taught with Zaid Harchaoui*  
Reviews optimization and convex optimization in its relation to statistics. Covers the basics of unconstrained and constrained convex optimization, basics of clustering and classification, entropy, KL divergence and exponential family models, duality, modern learning algorithms like boosting, support vector machines, and variational approximations in inference.

## Teaching Assistant  
**Convex Optimization**,  *2014-2017.*   
*Master Mathematics, Vision, Learning, École Normale Supérieure Paris-Saclay, Paris.*  
Taught by [Alexandre d'Aspremont](https://www.di.ens.fr/~aspremon)   
[website](https://www.di.ens.fr/~aspremon/OptConvexeM2.html)  

**Oral Interrogations in Maths**,  *2013-2014*   
*Classes Préparatoires in Mathematics and Physics Lycée Janson de Sailly, Paris.*  

## Tutorials  
**Automatic Differentiation**,
*Statistical Machine Learning for Data Scientists, University of Washington, Seattle.*     
Lecture on automatic differentiation with code examples covering: how to compute gradients of
a chain of computations, how to use automatic-differentiation software, how to use automatic-
differentiation beyond gradient computations.  
[slides](/files/auto_diff_tuto.pdf)
[notebook](/files/auto_diff_tuto.ipynb)

**Optimization for deep learning**,  *Jul. 2018.*  
*Summer School on Fundamentals of Data Analysis,University of Wisconsin-Madison, Madison.*    
Interactive Jupyter Notebook to understand the basics of optimization for
deep learning: automatic-differentiation, convergence guarantees of SGD, illustration of the
batch-normalization effect.   
<!-- [notes](/files/lab1_optimization_notes.pdf)
[notebook](/files/lab1_optimization_deep_learning.ipynb) -->
