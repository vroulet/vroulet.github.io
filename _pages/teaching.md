---
layout: archive
title: "Teaching"
author_profile: true
redirect_from:
  - /teaching.html
---

{% include base_path %}

## Lecturer  
**Probability II**,  *Spring 2020.*  
*University of Washington, Seattle.*  
Random variables; expectation and variance; laws of large numbers; normal approximation and
other limit theorems; multidimensional distributions and transformations.   

### Lecture slides
Lecture 5: Joint distribution discrete case,  marginals, multinomial distribution    
[Slides](/files/lec5.pdf)       
Lecture 6: Joint distribution continuous case,  marginals, uniform distribution   
[Slides](/files/lec6.pdf)    
Lecture 7: Joint distribution and independence, consequences in p.m.f., p.d.f    
[Slides](/files/lec7.pdf)    


### Lecture notes
Lectures 1 to 4: [Review of MATH/STAT394: Introduction to Probability](/files/week1.pdf)    
Lectures 5 to 7: [Joint Probability Distributions, Independence](/files/week2.pdf)   

### Homeworks
[Homework 1](/files/hw1.pdf)


**Statistical Learning**,  *Winter 2020.*  
*University of Washington, Seattle.*  
Reviews optimization and convex optimization in its relation to statistics. Covers the basics
of unconstrained and constrained convex optimization, basics of clustering and classification,
entropy, KL divergence and exponential family models, duality, modern learning algorithms like
boosting, support vector machines, and variational approximations in inference.

## Teaching Assistant  
**Convex Optimization**,  *2014-2017.*   
*Master Mathematics, Vision, Learning, École Normale Supérieure Paris-Saclay, Paris.*  
Taught by [Alexandre d'Aspremont](https://www.di.ens.fr/~aspremon)   
[website](https://www.di.ens.fr/~aspremon/OptConvexeM2.html)  

**Oral Interrogations in Maths**,  *2013-2014*   
*Classes Préparatoires in Mathematics and Physics Lycée Janson de Sailly, Paris.*  

## Tutorials  
**Automatic Differentiation**,  *May 2019.*  
*Statistical Machine Learning for Data Scientists, University of Washington, Seattle.*     
Lecture on automatic differentiation with code examples covering: how to compute gradients of
a chain of computations, how to use automatic-differentiation software, how to use automatic-
differentiation beyond gradient computations.  
[slides](/files/autodiff.pdf)

**Optimization for deep learning**,  *Jul. 2018.*  
*Summer School on Fundamentals of Data Analysis,University of Wisconsin-Madison, Madison.*    
Interactive Jupyter Notebook for 30 attendees to understand the basics of optimization for
deep learning: automatic-differentiation, convergence guarantees of SGD, illustration of the
batch-normalization effect.
