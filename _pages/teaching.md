---
layout: archive
title: "Teaching"
author_profile: true
redirect_from:
  - /teaching.html
---

{% include base_path %}
##Lecturer  
**Probability II**  
*University of Washington, Seattle, Spring 2020.*  
Random variables; expectation and variance; laws of large numbers; normal approximation and
other limit theorems; multidimensional distributions and transformations.  

**Statistical Learning**  
*University of Washington, Seattle, Winter 2020*  
Reviews optimization and convex optimization in its relation to statistics. Covers the basics
of unconstrained and constrained convex optimization, basics of clustering and classification,
entropy, KL divergence and exponential family models, duality, modern learning algorithms like
boosting, support vector machines, and variational approximations in inference.

## Teaching Assistant  
**Convex Optimization**  
*Master Mathematics, Vision, Learning, École Normale Supérieure Paris-Saclay, Paris, 2014-2017.*  
From 2014 to 2017, I was a teaching assistant for the introduction to [convex optimization](https://www.di.ens.fr/~aspremon/OptConvexeM2.html) provided by [Alexandre d'Aspremont](https://www.di.ens.fr/~aspremon) for the master [Mathématiques Vision et Apprentissage](http://math.ens-paris-saclay.fr/version-francaise/formations/master-mva/) in Paris.

**Oral Interrogations in Maths**  
*Classes Préparatoires in Mathematics and Physics,
Lycée Janson de Sailly, Paris, 2013-2014.*  

## Tutorials
**Automatic Differentiation**  
*Statistical Machine Learning for Data Scientists, University of Washington, 2019.*  
Lecture on automatic differentiation with code examples covering: how to compute gradients of
a chain of computations, how to use automatic-differentiation software, how to use automatic-
differentiation beyond gradient computations.
[slides](/files/autodiff.pdf)

**Optimization for deep learning**  
*Summer School on Fundamentals of Data Analysis, University of Wisconsin-Madison, 2018.*  
Interactive Jupyter Notebook for 30 attendees to understand the basics of optimization for
deep learning: automatic-differentiation, convergence guarantees of SGD, illustration of the
batch-normalization effect.
