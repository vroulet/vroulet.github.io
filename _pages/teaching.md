---
layout: archive
title: "Teaching"
author_profile: true
redirect_from:
  - /teaching.html
---

{% include base_path %}

## Lecturer  
**Probability II**,  *Spring 2020.*  
*University of Washington, Seattle.*  
This is the second quarter of a sequence in probability theory. This quarter, we study jointly distributed probability distributions, independent random variables, conditional distributions. We  also cover diverse representations of probability distributions beyond density and cumulative distributions, namely, we introduce moment generating functions. We then study the convergence of random variables and in particular the central limit theorem.
[Lecture slides](/files/mathstat395_lec1-19.pdf)   
[Lecture notes](/files/mathstat395_lecture_notes_1-6.pdf)   
[Homeworks](/files/mathstat395_homeworks1-4.pdf)    


**Statistical Learning: Modeling, Prediction, And Computing**,  *Winter 2020.*  
*University of Washington, Seattle. Co-taught with Zaid Harchaoui*  
The course presents advanced statistical machine learning methods from a functional estimation (nonparametric statistics) viewpoint. The course covers the theoretical analysis of kernel-based methods, as well as their practical implementation using gradient-based optimization algorithms and numerical linear algebra algorithms. The course also covers an introduction to recent theoretical analyses of deep networks.

## Teaching Assistant  
**Convex Optimization**,  *2014-2017.*   
*Master Mathematics, Vision, Learning, École Normale Supérieure Paris-Saclay, Paris.*  
Taught by [Alexandre d'Aspremont](https://www.di.ens.fr/~aspremon)   
[website](https://www.di.ens.fr/~aspremon/OptConvexeM2.html)  

**Oral Interrogations in Maths**,  *2013-2014*   
*Classes Préparatoires in Mathematics and Physics Lycée Janson de Sailly, Paris.*  

## Tutorials  
**Automatic Differentiation**,  *May 2019.*  
*Statistical Machine Learning for Data Scientists, University of Washington, Seattle.*     
Lecture on automatic differentiation with code examples covering: how to compute gradients of
a chain of computations, how to use automatic-differentiation software, how to use automatic-
differentiation beyond gradient computations.  
[slides](/files/autodiff.pdf)

**Optimization for deep learning**,  *Jul. 2018.*  
*Summer School on Fundamentals of Data Analysis,University of Wisconsin-Madison, Madison.*    
Interactive Jupyter Notebook for 30 attendees to understand the basics of optimization for
deep learning: automatic-differentiation, convergence guarantees of SGD, illustration of the
batch-normalization effect.   
[notes](/files/lab1_optimization_notes.pdf)  
[notebook](/files/lab1_optimization_deep_learning.ipynb)
